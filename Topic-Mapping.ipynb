{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Report-large**\n",
        "Title + Desc\n",
        "\n",
        "*  trying on: 10 , 20 30 , 40\n",
        "\n",
        "*   Best result cat 1 Personal: **10**\n",
        "*   Best result cat 2 Physical: **30**\n",
        "*   Best result cat 3 Cybersecurity: **40**\n",
        "\n",
        "\n",
        "calculating the accuracy\n",
        "\n",
        "\n",
        "\n",
        "*   input -  keywords_with_generated_topic# (from-BestNaming) + LabeledDataset_target_#\n",
        "*   Output - rule_mapping#_with_preprocessed_keywords_#_NOACTION_LM.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "jlsFst3u_MLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lib"
      ],
      "metadata": {
        "id": "tGFokBdIEzbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snl44_SKE2ce",
        "outputId": "b3e97b2d-fd12-4c26-964c-8d6183f587f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Data\n",
        "data_target_1 = {\n",
        "    'Dataset': ['10 Topic', '20 Topic', '30 Topic', '40 Topic'],\n",
        "    'Average Similarity': [0.992455, 0.992143, 0.992455, 0.991591]\n",
        "}\n",
        "\n",
        "data_target_2 = {\n",
        "    'Dataset': ['10 Topic', '20 Topic', '30 Topic', '40 Topic'],\n",
        "    'Average Similarity': [0.992314, 0.992221, 0.992672, 0.992544]\n",
        "}\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Data\n",
        "data_target_1 = {\n",
        "    'Dataset': ['10 Topic', '20 Topic', '30 Topic', '40 Topic'],\n",
        "    'Average Similarity': [0.992455, 0.992143, 0.992455, 0.991591]\n",
        "}\n",
        "\n",
        "data_target_2 = {\n",
        "    'Dataset': ['10 Topic', '20 Topic', '30 Topic', '40 Topic'],\n",
        "    'Average Similarity': [0.992314, 0.992221, 0.992672, 0.992544]\n",
        "}\n",
        "\n",
        "data_target_3 = {\n",
        "    'Dataset': ['10 Topic', '20 Topic', '30 Topic', '40 Topic'],\n",
        "    'Average Similarity': [0.991674, 0.992222, 0.992462, 0.992490]\n",
        "}\n",
        "\n",
        "# Create dataframes\n",
        "df_target_1 = pd.DataFrame(data_target_1)\n",
        "df_target_2 = pd.DataFrame(data_target_2)\n",
        "df_target_3 = pd.DataFrame(data_target_3)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Bar width\n",
        "width = 0.15\n",
        "x = range(4)  # 4 bars per target (10, 20, 30, 40 topics)\n",
        "\n",
        "# Set x positions for each target\n",
        "x_target_1 = [i - width * 1.5 for i in x]\n",
        "x_target_2 = [i - width * 0.5 for i in x]\n",
        "x_target_3 = [i + width * 0.5 for i in x]\n",
        "x_target_4 = [i + width * 1.5 for i in x]\n",
        "\n",
        "# Bar plot for each dataset with specific colors for each target\n",
        "ax.bar(x_target_1, df_target_1['Average Similarity'], width=width, label='10 Topic', color='blue')\n",
        "ax.bar(x_target_2, df_target_2['Average Similarity'], width=width, label='20 Topic', color='purple')\n",
        "ax.bar(x_target_3, df_target_3['Average Similarity'], width=width, label='30 Topic', color='grey')\n",
        "ax.bar(x_target_4, df_target_1['Average Similarity'], width=width, label='40 Topic', color='darkblue')\n",
        "\n",
        "# Labeling\n",
        "ax.set_xlabel('Target')\n",
        "ax.set_ylabel('Average Similarity')\n",
        "ax.set_title('Comparison of Datasets: Average Similarity for Target 1, Target 2, and Target 3')\n",
        "\n",
        "# Adjusting the x-axis positions for targets\n",
        "ax.set_xticks([i for i in range(4)])  # Set x positions for the targets\n",
        "ax.set_xticklabels(['10 Topic', '20 Topic', '30 Topic', '40 Topic'])  # Topics for each target\n",
        "\n",
        "# Adjusting the y-axis range to 0.99 to 1\n",
        "ax.set_ylim(0.99, 1)\n",
        "\n",
        "# Adding legends\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "data_target_3 = {\n",
        "    'Dataset': ['10 Topic', '20 Topic', '30 Topic', '40 Topic'],\n",
        "    'Average Similarity': [0.991674, 0.992222, 0.992462, 0.992490]\n",
        "}\n",
        "\n",
        "# Create dataframes\n",
        "df_target_1 = pd.DataFrame(data_target_1)\n",
        "df_target_2 = pd.DataFrame(data_target_2)\n",
        "df_target_3 = pd.DataFrame(data_target_3)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Bar width\n",
        "width = 0.15\n",
        "x = range(4)  # 4 bars per target (10, 20, 30, 40 topics)\n",
        "\n",
        "# Set x positions for each target\n",
        "x_target_1 = [i - width * 1.5 for i in x]\n",
        "x_target_2 = [i - width * 0.5 for i in x]\n",
        "x_target_3 = [i + width * 0.5 for i in x]\n",
        "x_target_4 = [i + width * 1.5 for i in x]\n",
        "\n",
        "# Bar plot for each dataset with specific colors for each target\n",
        "ax.bar(x_target_1, df_target_1['Average Similarity'], width=width, label='10 Topic', color='blue')\n",
        "ax.bar(x_target_2, df_target_2['Average Similarity'], width=width, label='20 Topic', color='purple')\n",
        "ax.bar(x_target_3, df_target_3['Average Similarity'], width=width, label='30 Topic', color='grey')\n",
        "ax.bar(x_target_4, df_target_1['Average Similarity'], width=width, label='40 Topic', color='darkblue')\n",
        "\n",
        "# Labeling\n",
        "ax.set_xlabel('Target')\n",
        "ax.set_ylabel('Average Similarity')\n",
        "ax.set_title('Comparison of Datasets: Average Similarity for Target 1, Target 2, and Target 3')\n",
        "\n",
        "# Adjusting the x-axis positions for targets\n",
        "ax.set_xticks([i for i in range(4)])  # Set x positions for the targets\n",
        "ax.set_xticklabels(['10 Topic', '20 Topic', '30 Topic', '40 Topic'])  # Topics for each target\n",
        "\n",
        "# Adjusting the y-axis range to 0.99 to 1\n",
        "ax.set_ylim(0.99, 1)\n",
        "\n",
        "# Adding legends\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "FNLTXls5QLY_",
        "outputId": "8e36e183-7274-4329-84bf-4412d28068d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cat 1 mapping  **Personal**"
      ],
      "metadata": {
        "id": "y9QltTWaDavl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 10 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):  # Handle NaN or float cases\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())  # Remove special characters\n",
        "    words = word_tokenize(text)  # Tokenize the words\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    # Lemmatize the words\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic1_names_10_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_1.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "r5yvthI-UY7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 20 .. Mapp\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic1_names_20_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_1.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping1_with_preprocessed_keywords_20_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "nR1dDz4eUbAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 30 .. Mapp\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic1_names_30_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_1.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping1_with_preprocessed_keywords_30_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "z0Gsvx5xUclD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 40 .. Mapp\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic1_names_40_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_1.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping1_with_preprocessed_keywords_40_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "Vk2q4DgDUeTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy C1"
      ],
      "metadata": {
        "id": "NFdMQnd-ZocZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall current pandas\n",
        "!pip uninstall -y pandas\n",
        "\n",
        "# Install the compatible version of pandas\n",
        "!pip install pandas==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAkq6gdvd0wb",
        "outputId": "16878afb-c89c-4e1a-dd39-04034f0406de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Collecting pandas==2.2.2\n",
            "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
            "Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Installing collected packages: pandas\n",
            "Successfully installed pandas-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall pandas and numpy\n",
        "!pip uninstall -y pandas numpy\n",
        "\n",
        "# Install specific compatible versions\n",
        "!pip install numpy==1.23.5 pandas==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "MXD9pFrze0Ee",
        "outputId": "610c5484-8766-4cb3-c16d-413c2cc0554d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: numpy 2.1.2\n",
            "Uninstalling numpy-2.1.2:\n",
            "  Successfully uninstalled numpy-2.1.2\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
            "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Installing collected packages: numpy, pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.16 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.24.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5 pandas-2.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "8ae08d1fb6ac4390a5794b95383bebd7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Function to upload files\n",
        "def upload_files():\n",
        "    uploaded = files.upload()\n",
        "    return uploaded\n",
        "\n",
        "# Load datasets with error handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        dataset = pd.read_csv(file_path)\n",
        "        if dataset.empty:\n",
        "            print(f\"{file_path} is empty.\")\n",
        "            return None\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Upload files\n",
        "upload_files()\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = load_dataset('rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv')\n",
        "dataset2 = load_dataset('rule_mapping1_with_preprocessed_keywords_20_NOACTION_LM.csv')\n",
        "dataset3 = load_dataset('rule_mapping1_with_preprocessed_keywords_30_NOACTION_LM.csv')\n",
        "dataset4 = load_dataset('rule_mapping1_with_preprocessed_keywords_40_NOACTION_LM.csv')\n",
        "\n",
        "# Function to analyze each dataset\n",
        "def analyze_dataset(dataset):\n",
        "    if dataset is None:\n",
        "        return None, None\n",
        "\n",
        "    similarity_column = 'Similarity Score' in dataset.columns\n",
        "    match_quality_column = 'Match Quality' in dataset.columns\n",
        "\n",
        "    average_similarity = dataset['Similarity Score'].mean() if similarity_column else None\n",
        "    match_quality = dataset['Match Quality'].value_counts() if match_quality_column else None\n",
        "\n",
        "    if match_quality is not None and match_quality.sum() > 0:\n",
        "        accuracy = (match_quality.get('High Confidence', 0) / match_quality.sum() * 100)\n",
        "    else:\n",
        "        accuracy = None\n",
        "\n",
        "    return average_similarity, accuracy\n",
        "\n",
        "# Analyze all datasets\n",
        "avg_similarity_1, accuracy_1 = analyze_dataset(dataset1)\n",
        "avg_similarity_2, accuracy_2 = analyze_dataset(dataset2)\n",
        "avg_similarity_3, accuracy_3 = analyze_dataset(dataset3)\n",
        "avg_similarity_4, accuracy_4 = analyze_dataset(dataset4)\n",
        "\n",
        "# Create a DataFrame to compare results\n",
        "results = pd.DataFrame({\n",
        "    'Dataset': ['Dataset 1', 'Dataset 2', 'Dataset 3', 'Dataset 4'],\n",
        "    'Average Similarity': [avg_similarity_1, avg_similarity_2, avg_similarity_3, avg_similarity_4],\n",
        "    'Accuracy (%)': [accuracy_1, accuracy_2, accuracy_3, accuracy_4]\n",
        "})\n",
        "\n",
        "# Determine the best dataset based on average similarity score\n",
        "best_dataset = results.loc[results['Average Similarity'].idxmax()]\n",
        "\n",
        "# Print results\n",
        "print(\"\\nComparison of Datasets:\")\n",
        "print(results)\n",
        "\n",
        "print(f\"\\nThe best dataset to use is: {best_dataset['Dataset']} \"\n",
        "      f\"with an average similarity score of {best_dataset['Average Similarity']} \"\n",
        "      f\"and accuracy of {best_dataset['Accuracy (%)']}%.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "dRa2s9IQc2E3",
        "outputId": "0ae47e84-a658-403f-fdcb-36f8e9b812c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98c11366-b459-489c-ad77-4e7d84ee9058\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98c11366-b459-489c-ad77-4e7d84ee9058\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving rule_mapping_with_preprocessed_keywords_10_NOACTION_LM.csv to rule_mapping_with_preprocessed_keywords_10_NOACTION_LM (1).csv\n",
            "Saving rule_mapping_with_preprocessed_keywords_20_NOACTION_LM.csv to rule_mapping_with_preprocessed_keywords_20_NOACTION_LM (1).csv\n",
            "Saving rule_mapping_with_preprocessed_keywords_30_NOACTION_LM.csv to rule_mapping_with_preprocessed_keywords_30_NOACTION_LM (1).csv\n",
            "Saving rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv to rule_mapping_with_preprocessed_keywords_40_NOACTION_LM (1).csv\n",
            "Saving rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv to rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM (2).csv\n",
            "Saving rule_mapping1_with_preprocessed_keywords_20_NOACTION_LM.csv to rule_mapping1_with_preprocessed_keywords_20_NOACTION_LM (2).csv\n",
            "Saving rule_mapping1_with_preprocessed_keywords_30_NOACTION_LM.csv to rule_mapping1_with_preprocessed_keywords_30_NOACTION_LM (2).csv\n",
            "Saving rule_mapping1_with_preprocessed_keywords_40_NOACTION_LM.csv to rule_mapping1_with_preprocessed_keywords_40_NOACTION_LM (2).csv\n",
            "Saving rule_mapping2_with_preprocessed_keywords_10_NOACTION_LM.csv to rule_mapping2_with_preprocessed_keywords_10_NOACTION_LM (1).csv\n",
            "Saving rule_mapping2_with_preprocessed_keywords_20_NOACTION_LM.csv to rule_mapping2_with_preprocessed_keywords_20_NOACTION_LM (1).csv\n",
            "Saving rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv to rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM (1).csv\n",
            "Saving rule_mapping2_with_preprocessed_keywords_40_NOACTION_LM.csv to rule_mapping2_with_preprocessed_keywords_40_NOACTION_LM (1).csv\n",
            "\n",
            "Comparison of Datasets:\n",
            "     Dataset  Average Similarity  Accuracy (%)\n",
            "0  Dataset 1            0.992455         100.0\n",
            "1  Dataset 2            0.992143         100.0\n",
            "2  Dataset 3            0.992455         100.0\n",
            "3  Dataset 4            0.991591           0.0\n",
            "\n",
            "The best dataset to use is: Dataset 1 with an average similarity score of 0.992455261986578 and accuracy of 100.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cat 2 mapping **Physical**"
      ],
      "metadata": {
        "id": "xLMjie1iDhsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 10 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic2_names_10_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_2.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping2_with_preprocessed_keywords_10_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "uw2VA4FnYk_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 20 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic2_names_20_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_2.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping2_with_preprocessed_keywords_20_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "Q0QhCRRWYlwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 30 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic2_names_30_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_2.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "fhAcx-FoYmjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 40 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic2_names_40_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_2.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping2_with_preprocessed_keywords_40_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "vU5rLCuCY5fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accurcy C2"
      ],
      "metadata": {
        "id": "Jfz203nDa7AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets with error handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all datasets\n",
        "dataset1 = load_dataset('rule_mapping2_with_preprocessed_keywords_10_NOACTION_LM.csv')\n",
        "dataset2 = load_dataset('rule_mapping2_with_preprocessed_keywords_20_NOACTION_LM.csv')\n",
        "dataset3 = load_dataset('rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv')\n",
        "dataset4 = load_dataset('rule_mapping2_with_preprocessed_keywords_40_NOACTION_LM.csv')\n",
        "\n",
        "# Function to analyze each dataset\n",
        "def analyze_dataset(dataset):\n",
        "    if dataset is None:\n",
        "        return None, None\n",
        "\n",
        "    similarity_column = 'Similarity Score' in dataset.columns\n",
        "    match_quality_column = 'Match Quality' in dataset.columns\n",
        "\n",
        "    average_similarity = dataset['Similarity Score'].mean() if similarity_column else None\n",
        "    match_quality = dataset['Match Quality'].value_counts() if match_quality_column else None\n",
        "\n",
        "    accuracy = (match_quality.get('High Confidence', 0) / match_quality.sum() * 100) if match_quality is not None and match_quality.sum() > 0 else None\n",
        "\n",
        "    return average_similarity, accuracy\n",
        "\n",
        "# Analyze all datasets\n",
        "avg_similarity_1, accuracy_1 = analyze_dataset(dataset1)\n",
        "avg_similarity_2, accuracy_2 = analyze_dataset(dataset2)\n",
        "avg_similarity_3, accuracy_3 = analyze_dataset(dataset3)\n",
        "avg_similarity_4, accuracy_4 = analyze_dataset(dataset4)\n",
        "\n",
        "# Create a DataFrame to compare results\n",
        "results = pd.DataFrame({\n",
        "    'Dataset': ['Dataset 1', 'Dataset 2', 'Dataset 3', 'Dataset 4'],\n",
        "    'Average Similarity': [avg_similarity_1, avg_similarity_2, avg_similarity_3, avg_similarity_4],\n",
        "    'Accuracy (%)': [accuracy_1, accuracy_2, accuracy_3, accuracy_4]\n",
        "})\n",
        "\n",
        "# Determine the best dataset based on average similarity score\n",
        "best_dataset = results.loc[results['Average Similarity'].idxmax()]\n",
        "\n",
        "# Print results\n",
        "print(\"\\nComparison of Datasets:\")\n",
        "print(results)\n",
        "\n",
        "print(f\"\\nThe best dataset to use is: {best_dataset['Dataset']} \"\n",
        "      f\"with an average similarity score of {best_dataset['Average Similarity']} \"\n",
        "      f\"and accuracy of {best_dataset['Accuracy (%)']}%.\")"
      ],
      "metadata": {
        "id": "qEvYvtTCa4LV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b83b12e-06e1-4ea5-ebab-f664180c6013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Datasets:\n",
            "     Dataset  Average Similarity  Accuracy (%)\n",
            "0  Dataset 1            0.992314         100.0\n",
            "1  Dataset 2            0.992221         100.0\n",
            "2  Dataset 3            0.992672         100.0\n",
            "3  Dataset 4            0.992544         100.0\n",
            "\n",
            "The best dataset to use is: Dataset 3 with an average similarity score of 0.9926722601636664 and accuracy of 100.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cat 3 mapping **Cybersecurity**"
      ],
      "metadata": {
        "id": "O3NSTPwo__2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 10 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic_names_10_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_3.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping_with_preprocessed_keywords_10_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEbTJIaBAZ74",
        "outputId": "164a66f3-a109-4472-c675-6d6c964bfca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name roberta-large. Creating a new one with mean pooling.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to Cat3_Mapping_keywords_10_topic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 20 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic_names_20_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_3.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping_with_preprocessed_keywords_20_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "u5kuF548Aa7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 30 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic_names_30_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_3.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping_with_preprocessed_keywords_30_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "c93hZUdyAb4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- .. 40 .. Mapp\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's English model for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to preprocess text: lowercase, remove punctuation, stopwords, and apply lemmatization\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = ''\n",
        "    text = re.sub(r'\\W', ' ', text.lower())\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "# Load datasets\n",
        "keywords_df = pd.read_csv('keywords_with_generated_topic_names_40_L.csv')\n",
        "labeled_dataset_df = pd.read_csv('LabeledDataset_target_3.csv')\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = SentenceTransformer('roberta-large')\n",
        "\n",
        "# Preprocess Keywords\n",
        "keywords_df['Processed Keywords'] = keywords_df['Keywords'].apply(preprocess_text)\n",
        "keywords = keywords_df['Processed Keywords'].tolist()\n",
        "\n",
        "# Fill missing values in fields before concatenation\n",
        "labeled_dataset_df.fillna({ 'title': '', 'desc': ''}, inplace=True)\n",
        "\n",
        "# Concatenate and preprocess title and description fields\n",
        "labeled_dataset_df['Title + Description'] = (\n",
        "    labeled_dataset_df['title'] + ' ' + labeled_dataset_df['desc']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to concatenated fields\n",
        "labeled_dataset_df['Processed Title + Description'] = labeled_dataset_df['Title + Description'].apply(preprocess_text)\n",
        "combined_fields = labeled_dataset_df['Processed Title + Description'].tolist()\n",
        "\n",
        "# Generate embeddings\n",
        "keywords_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "combined_embeddings = model.encode(combined_fields, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "cosine_scores = util.pytorch_cos_sim(keywords_embeddings, combined_embeddings)\n",
        "\n",
        "# Determine best matches and confidence scores\n",
        "best_matches = cosine_scores.argmax(dim=0).tolist()\n",
        "best_scores = cosine_scores.max(dim=0).values.tolist()\n",
        "\n",
        "# Add results to the labeled dataset\n",
        "labeled_dataset_df['Best Matched Keywords'] = [keywords[idx] for idx in best_matches]\n",
        "labeled_dataset_df['Similarity Score'] = best_scores\n",
        "labeled_dataset_df['Match Quality'] = ['High Confidence' if score >= 0.5 else 'Low Confidence' for score in best_scores]\n",
        "labeled_dataset_df['Generated Topic Name'] = [keywords_df.loc[idx, 'Generated Topic Name'] for idx in best_matches]\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = 'rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv'\n",
        "labeled_dataset_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "HTMkXP0rAqvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accurcy C3"
      ],
      "metadata": {
        "id": "-VraYwT7BZF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE Best numbur of topic\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets with error handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all datasets\n",
        "dataset1 = load_dataset('rule_mapping_with_preprocessed_keywords_10_NOACTION_LM.csv')\n",
        "dataset2 = load_dataset('rule_mapping_with_preprocessed_keywords_20_NOACTION_LM.csv')\n",
        "dataset3 = load_dataset('rule_mapping_with_preprocessed_keywords_30_NOACTION_LM.csv')\n",
        "dataset4 = load_dataset('rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv')\n",
        "\n",
        "# Function to analyze each dataset\n",
        "def analyze_dataset(dataset):\n",
        "    if dataset is None:\n",
        "        return None, None\n",
        "\n",
        "    similarity_column = 'Similarity Score' in dataset.columns\n",
        "    match_quality_column = 'Match Quality' in dataset.columns\n",
        "\n",
        "    average_similarity = dataset['Similarity Score'].mean() if similarity_column else None\n",
        "    match_quality = dataset['Match Quality'].value_counts() if match_quality_column else None\n",
        "\n",
        "    accuracy = (match_quality.get('High Confidence', 0) / match_quality.sum() * 100) if match_quality is not None and match_quality.sum() > 0 else None\n",
        "\n",
        "    return average_similarity, accuracy\n",
        "\n",
        "# Analyze all datasets\n",
        "avg_similarity_1, accuracy_1 = analyze_dataset(dataset1)\n",
        "avg_similarity_2, accuracy_2 = analyze_dataset(dataset2)\n",
        "avg_similarity_3, accuracy_3 = analyze_dataset(dataset3)\n",
        "avg_similarity_4, accuracy_4 = analyze_dataset(dataset4)\n",
        "\n",
        "# Create a DataFrame to compare results\n",
        "results = pd.DataFrame({\n",
        "    'Dataset': ['Dataset 1', 'Dataset 2', 'Dataset 3', 'Dataset 4'],\n",
        "    'Average Similarity': [avg_similarity_1, avg_similarity_2, avg_similarity_3, avg_similarity_4],\n",
        "    'Accuracy (%)': [accuracy_1, accuracy_2, accuracy_3, accuracy_4]\n",
        "})\n",
        "\n",
        "# Determine the best dataset based on average similarity score\n",
        "best_dataset = results.loc[results['Average Similarity'].idxmax()]\n",
        "\n",
        "# Print results\n",
        "print(\"\\nComparison of Datasets:\")\n",
        "print(results)\n",
        "\n",
        "print(f\"\\nThe best dataset to use is: {best_dataset['Dataset']} \"\n",
        "      f\"with an average similarity score of {best_dataset['Average Similarity']} \"\n",
        "      f\"and accuracy of {best_dataset['Accuracy (%)']}%.\")"
      ],
      "metadata": {
        "id": "jiIIkJWxBdH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2703343e-0549-4d00-b07f-1734c473239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Datasets:\n",
            "     Dataset  Average Similarity  Accuracy (%)\n",
            "0  Dataset 1            0.991674         100.0\n",
            "1  Dataset 2            0.992222         100.0\n",
            "2  Dataset 3            0.992462         100.0\n",
            "3  Dataset 4            0.992490         100.0\n",
            "\n",
            "The best dataset to use is: Dataset 4 with an average similarity score of 0.9924899582278022 and accuracy of 100.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combined Dataset**"
      ],
      "metadata": {
        "id": "xgWXwVGmnvDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combining\n",
        "#rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv\n",
        "#rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv\n",
        "#rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets with error handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "# Load datasets\n",
        "dataset1 = load_dataset('rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv') #cat 1\n",
        "dataset2 = load_dataset('rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv') #cat 2\n",
        "dataset3 = load_dataset('rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv')  #cat 3\n",
        "\n",
        "# Combine datasets by concatenation\n",
        "combined_dataset = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
        "\n",
        "# Optionally, drop duplicates if needed\n",
        "combined_dataset.drop_duplicates(inplace=True)\n",
        "\n",
        "# Save the output to a CSV file\n",
        "combined_dataset.to_csv('combined_dataset.csv', index=False)\n",
        "\n",
        "print(\"Combined dataset saved as 'combined_dataset.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex4Maqtnn71E",
        "outputId": "04ab3f8c-1aed-483e-a343-8fd5ca8689cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset saved as 'combined_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combining\n",
        "#rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv\n",
        "#rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv\n",
        "#rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets with error handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = load_dataset('rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv')\n",
        "dataset2 = load_dataset('rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv')\n",
        "dataset3 = load_dataset('rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv')\n",
        "\n",
        "# Combine datasets by concatenation\n",
        "combined_dataset = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
        "\n",
        "# Optionally, drop duplicates if needed\n",
        "combined_dataset.drop_duplicates(inplace=True)\n",
        "\n",
        "# Split the dataset into 75% for training and 25% for testing\n",
        "train_size = int(0.75 * len(combined_dataset))\n",
        "combined_dataset_training = combined_dataset[:train_size]\n",
        "combined_dataset_testing = combined_dataset[train_size:]\n",
        "\n",
        "# Save the training and testing datasets to CSV files\n",
        "combined_dataset_training.to_csv('combined_dataset_training.csv', index=False)\n",
        "combined_dataset_testing.to_csv('combined_dataset_testing.csv', index=False)\n",
        "\n",
        "print(\"Training and testing datasets saved as 'combined_dataset_training.csv' and 'combined_dataset_testing.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gup5LqO0yiO",
        "outputId": "48c037f6-9746-47c9-ce02-c46d47f9a48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and testing datasets saved as 'combined_dataset_training.csv' and 'combined_dataset_testing.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#correct split of thedataset random\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load datasets with error handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = load_dataset('rule_mapping1_with_preprocessed_keywords_10_NOACTION_LM.csv')\n",
        "dataset2 = load_dataset('rule_mapping2_with_preprocessed_keywords_30_NOACTION_LM.csv')\n",
        "dataset3 = load_dataset('rule_mapping_with_preprocessed_keywords_40_NOACTION_LM.csv')\n",
        "\n",
        "# Combine datasets by concatenation\n",
        "combined_dataset = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
        "\n",
        "# Optionally, drop duplicates if needed\n",
        "combined_dataset.drop_duplicates(inplace=True)\n",
        "\n",
        "# Split the dataset into 75% for training and 25% for testing, randomly\n",
        "combined_dataset_training, combined_dataset_testing = train_test_split(combined_dataset, test_size=0.25, random_state=42)\n",
        "\n",
        "# Save the training and testing datasets to CSV files\n",
        "combined_dataset_training.to_csv('combined_dataset_75training.csv', index=False)\n",
        "combined_dataset_testing.to_csv('combined_dataset_25testing.csv', index=False)\n",
        "\n",
        "print(\"Training and testing datasets saved as 'combined_dataset_training.csv' and 'combined_dataset_testing.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r00607SoYX8C",
        "outputId": "67c8339e-37eb-4c7a-d33c-f34d94aa3970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and testing datasets saved as 'combined_dataset_training.csv' and 'combined_dataset_testing.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XIbBKDfvn54y"
      }
    }
  ]
}
