{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN train rule to cve without cluster"
      ],
      "metadata": {
        "id": "bwhP4u88WpB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "embedding_dim = 300\n",
        "similarity_threshold = 0.6\n",
        "keyword_weight = 0.2\n",
        "\n",
        "# Loading the datasets\n",
        "train_data = pd.read_csv('combined_dataset_75training.csv')\n",
        "cve_data = pd.read_csv('Processed_CVE_withSpace.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_text = (\n",
        "    train_data['triggerChannelTitle'].fillna('') + \" \" +\n",
        "    train_data['actionChannelTitle'].fillna('') + \" \" +\n",
        "    train_data['Processed Title + Description'].fillna('') + \" \" +\n",
        "    train_data['Generated Topic Name'].fillna('') + \" \" +\n",
        "    train_data['Best Matched Keywords'].fillna('')\n",
        ")\n",
        "\n",
        "cve_text = cve_data['Processed_Text'].fillna('')\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_text.values)\n",
        "tokenizer.fit_on_texts(cve_text.values)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text.values)\n",
        "cve_sequences = tokenizer.texts_to_sequences(cve_text.values)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "cve_padded = pad_sequences(cve_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Load the Word2Vec embeddings\n",
        "word2vec_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Create embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        if word in word2vec:\n",
        "            embedding_matrix[i] = word2vec[word]\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len,\n",
        "                            weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "conv_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
        "batch_norm1 = BatchNormalization()(conv_layer)\n",
        "global_pool = GlobalMaxPooling1D()(batch_norm1)\n",
        "dense_layer = Dense(64, activation='relu')(global_pool)\n",
        "batch_norm2 = BatchNormalization()(dense_layer)\n",
        "dropout_layer = Dropout(0.5)(batch_norm2)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "final_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "embedding_extractor = Model(inputs=final_model.input, outputs=global_pool)\n",
        "\n",
        "# Generate embeddings\n",
        "train_embeddings = embedding_extractor.predict(train_padded)\n",
        "cve_embeddings = embedding_extractor.predict(cve_padded)\n",
        "\n",
        "# Computeing keyword similarity\n",
        "def keyword_similarity(row_keywords, cve_keywords):\n",
        "    train_keywords_set = set(row_keywords.lower().split())\n",
        "    cve_keywords_set = set(cve_keywords.lower().split())\n",
        "    intersection = train_keywords_set.intersection(cve_keywords_set)\n",
        "    union = train_keywords_set.union(cve_keywords_set)\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "\n",
        "def compute_keyword_similarities(train_data, cve_data):\n",
        "    keyword_similarities = np.zeros((len(train_data), len(cve_data)))\n",
        "    for i, train_keywords in enumerate(train_data['Best Matched Keywords']):\n",
        "        for j, cve_keywords in enumerate(cve_data['Processed_Text']):\n",
        "            keyword_similarities[i, j] = keyword_similarity(train_keywords, cve_keywords)\n",
        "    return keyword_similarities\n",
        "\n",
        "keyword_similarities = compute_keyword_similarities(train_data, cve_data)\n",
        "\n",
        "# Combine embedding and keyword similarity\n",
        "combined_similarity = (1 - keyword_weight) * cosine_similarity(train_embeddings, cve_embeddings) + keyword_weight * keyword_similarities\n",
        "best_matches = combined_similarity.argmax(axis=1)\n",
        "best_scores = combined_similarity.max(axis=1)\n",
        "\n",
        "# Generate train labels based on the best scores\n",
        "train_labels = (best_scores >= similarity_threshold).astype(int)\n",
        "\n",
        "# Train the CNN model on the train data\n",
        "print(\"Training the CNN model...\")\n",
        "final_model.fit(train_padded, train_labels, epochs=8 , batch_size=32)\n",
        "\n",
        "# Saveing of the trained cnn model\n",
        "final_model.save(\"final_cnn_model_for_cve_mapping_0.2w.keras\")\n",
        "print(\"Final trained model saved as 'final_cnn_model_for_cve_mapping_0.2w.keras'.\")\n",
        "\n",
        "# calculate the Accuracy\n",
        "correct_predictions = (best_scores >= similarity_threshold).sum()\n",
        "accuracy = correct_predictions / len(best_scores) * 100\n",
        "print(f\"Accuracy based on similarity threshold ({similarity_threshold}): {accuracy:.2f}\")\n",
        "\n",
        "# Add predictions to output data\n",
        "train_output_data = train_data.copy()\n",
        "train_output_data['CVE Name'] = cve_data['Name'].iloc[best_matches].values\n",
        "train_output_data['CVE Processed Text'] = cve_data['Processed_Text'].iloc[best_matches].values\n",
        "train_output_data['Similarity Score'] = best_scores\n",
        "train_output_data['Label'] = train_labels\n",
        "\n",
        "# columns to include in the final output dataset\n",
        "meaningful_columns = [\n",
        "    'triggerTitle', 'triggerChannelTitle', 'actionTitle',\n",
        "    'title', 'desc', 'target', 'Best Matched Keywords', 'Generated Topic Name',\n",
        "    'CVE Name', 'CVE Processed Text', 'Similarity Score', 'Label'\n",
        "]\n",
        "train_output_data = train_output_data[meaningful_columns]\n",
        "\n",
        "# Save results in the output dataset\n",
        "train_output_data.to_csv(\"training_mapping_results_cnn_model_0.2w.csv\", index=False)\n",
        "print(\"Final mapping results saved to 'training_mapping_results_cnn_model_0.2w.csv'.\")"
      ],
      "metadata": {
        "id": "3x-mGRxUT-aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn test rule to cve without cluster"
      ],
      "metadata": {
        "id": "2dX8ahniUL2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "similarity_threshold = 0.8\n",
        "keyword_weight = 0.6\n",
        "\n",
        "# Load datasets\n",
        "test_data = pd.read_csv('combined_dataset_25testing.csv')\n",
        "cve_data = pd.read_csv('Processed_CVE_withSpace.csv')\n",
        "expert_data = pd.read_csv('Complete_Rule_to_CVE_Mapping_with_Full_Details.csv')\n",
        "\n",
        "# Preprocess text data\n",
        "test_text = (\n",
        "    test_data['triggerChannelTitle'].fillna('') + \" \" +\n",
        "    test_data['actionChannelTitle'].fillna('') + \" \" +\n",
        "    test_data['Processed Title + Description'].fillna('') + \" \" +\n",
        "    test_data['Generated Topic Name'].fillna('') + \" \" +\n",
        "    test_data['Best Matched Keywords'].fillna('')\n",
        ")\n",
        "cve_text = cve_data['Processed_Text'].fillna('')\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(test_text.values)\n",
        "tokenizer.fit_on_texts(cve_text.values)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text.values)\n",
        "cve_sequences = tokenizer.texts_to_sequences(cve_text.values)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "cve_padded = pad_sequences(cve_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Load trained CNN model\n",
        "model_path = \"final_cnn_model_for_cve_mapping_0.2w.keras\"\n",
        "final_model = load_model(model_path)\n",
        "\n",
        "# Extract embeddings\n",
        "embedding_extractor = Model(inputs=final_model.input, outputs=final_model.layers[-3].output)\n",
        "test_embeddings = embedding_extractor.predict(test_padded)\n",
        "cve_embeddings = embedding_extractor.predict(cve_padded)\n",
        "\n",
        "# Compute keyword similarity\n",
        "def keyword_similarity(row_keywords, cve_keywords):\n",
        "    train_keywords_set = set(row_keywords.lower().split())\n",
        "    cve_keywords_set = set(cve_keywords.lower().split())\n",
        "    intersection = train_keywords_set.intersection(cve_keywords_set)\n",
        "    union = train_keywords_set.union(cve_keywords_set)\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "# Calculate keyword similarities\n",
        "def compute_keyword_similarities(test_data, cve_data):\n",
        "    keyword_similarities = np.zeros((len(test_data), len(cve_data)))\n",
        "    for i, test_keywords in enumerate(test_data['Best Matched Keywords']):\n",
        "        for j, cve_keywords in enumerate(cve_data['Processed_Text']):\n",
        "            keyword_similarities[i, j] = keyword_similarity(test_keywords, cve_keywords)\n",
        "    return keyword_similarities\n",
        "\n",
        "keyword_similarities = compute_keyword_similarities(test_data, cve_data)\n",
        "\n",
        "# Combine embedding and keyword similarity\n",
        "combined_similarity = (1 - keyword_weight) * cosine_similarity(test_embeddings, cve_embeddings) + keyword_weight * keyword_similarities\n",
        "\n",
        "# Normalize combined similarity to 0-1 range\n",
        "combined_similarity = (combined_similarity - combined_similarity.min()) / (combined_similarity.max() - combined_similarity.min())\n",
        "best_matches = combined_similarity.argmax(axis=1)\n",
        "best_scores = combined_similarity.max(axis=1)\n",
        "\n",
        "# Generate labels based on similarity threshold\n",
        "test_labels = (best_scores >= similarity_threshold).astype(int)\n",
        "\n",
        "# Map test data to CVE dataset\n",
        "test_data['CVE Name'] = cve_data['Name'].iloc[best_matches].values\n",
        "test_data['CVE Processed Text'] = cve_data['Processed_Text'].iloc[best_matches].values\n",
        "test_data['Similarity Score'] = best_scores\n",
        "test_data['Predicted Label'] = test_labels\n",
        "\n",
        "# Save the results\n",
        "test_data.to_csv(\"test_mapping_results_cnn_model_expert_without_cluster.csv\", index=False)\n",
        "print(\"Test mapping results saved to 'test_mapping_results_cnn_model_try.csv'.\")\n",
        "\n",
        "# Use the expert labels\n",
        "expert_labels = expert_data['Logical Match']\n",
        "\n",
        "\n",
        "if len(test_labels) != len(expert_labels):\n",
        "    raise ValueError(\"Mismatch in row counts between test data and expert-labeled data.\")\n",
        "\n",
        "\n",
        "print(\"Comparison with Expert Labels:\")\n",
        "print(classification_report(test_labels, expert_labels))\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(test_labels, expert_labels)\n",
        "print(f\"Accuracy Compared to Expert Labels: {accuracy * 100:.2f}%\")\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "\n",
        "precision = precision_score(expert_labels, test_labels, average='weighted')\n",
        "recall = recall_score(expert_labels, test_labels, average='weighted')\n",
        "f1 = f1_score(expert_labels, test_labels, average='weighted')\n",
        "accuracy = accuracy_score(expert_labels, test_labels)\n",
        "\n",
        "print(f\"Precision based on labels: {precision:.2f}\")\n",
        "print(f\"Recall based on labels: {recall:.2f}\")\n",
        "print(f\"F1-Score based on labels: {f1:.2f}\")\n",
        "print(f\"Accuracy based on labels: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Bb7g6rKMUhRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn train rule to cve with cluster"
      ],
      "metadata": {
        "id": "7bft26PoUkQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pickle\n",
        "\n",
        "# Load the datasets\n",
        "combined_df = pd.read_csv('combined_dataset_75training.csv')\n",
        "cve_df = pd.read_csv('Processed_CVE_withSpace.csv')\n",
        "\n",
        "# Ensure all entries are strings\n",
        "combined_df['Best Matched Keywords'] = combined_df['Best Matched Keywords'].fillna(\"\").astype(str)\n",
        "combined_df['Processed Title + Description'] = combined_df['Processed Title + Description'].fillna(\"\").astype(str)\n",
        "combined_df['actionChannelTitle'] = combined_df['actionChannelTitle'].fillna(\"\").astype(str)\n",
        "combined_df['triggerChannelTitle'] = combined_df['triggerChannelTitle'].fillna(\"\").astype(str)\n",
        "cve_df['Processed_Text'] = cve_df['Processed_Text'].fillna(\"\").astype(str)\n",
        "\n",
        "# Combine columns for rule text\n",
        "combined_texts = (\n",
        "    combined_df['Best Matched Keywords'] + \" \" +\n",
        "    combined_df['Processed Title + Description'] + \" \" +\n",
        "    combined_df['actionChannelTitle'] + \" \" +\n",
        "    combined_df['triggerChannelTitle']\n",
        ").tolist()\n",
        "\n",
        "cve_texts = cve_df['Processed_Text'].tolist()\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "embedding_dim = 300\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(combined_texts + cve_texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Load Word2Vec embeddings\n",
        "word2vec_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Create embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        if word in word2vec:\n",
        "            embedding_matrix[i] = word2vec[word]\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len,\n",
        "                            weights=[embedding_matrix], trainable=True)(input_layer)\n",
        "conv_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
        "batch_norm1 = BatchNormalization()(conv_layer)\n",
        "global_pool = GlobalMaxPooling1D()(batch_norm1)\n",
        "dense_layer = Dense(128, activation='relu')(global_pool)\n",
        "batch_norm2 = BatchNormalization()(dense_layer)\n",
        "dropout_layer = Dropout(0.5)(batch_norm2)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "# Compile the CNN model\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the embedding extractor model\n",
        "embedding_extractor = Model(inputs=cnn_model.input, outputs=global_pool)\n",
        "\n",
        "# Function to compute embeddings using CNN\n",
        "def compute_cnn_embeddings(texts, model, max_length=100):\n",
        "    # Tokenize and pad texts\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = model.predict(padded_sequences)\n",
        "    return embeddings\n",
        "\n",
        "# Compute embeddings\n",
        "print(\"Computing embeddings using CNN...\")\n",
        "combined_embeddings = compute_cnn_embeddings(combined_texts, embedding_extractor)\n",
        "cve_embeddings = compute_cnn_embeddings(cve_texts, embedding_extractor)\n",
        "print(\"Embeddings computed successfully.\")\n",
        "\n",
        "# Save the CNN model\n",
        "cnn_model.save(\"cnn_model_with_clustering_final28.keras\")\n",
        "print(\"CNN model saved to 'cnn_model_with_clustering.keras'.\")\n",
        "\n",
        "# Perform clustering using KMeans\n",
        "optimal_clusters = 20\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "cve_clusters = kmeans.fit_predict(cve_embeddings)\n",
        "print(f\"Clustering completed with {optimal_clusters} clusters.\")\n",
        "\n",
        "\n",
        "\n",
        "# Save the KMeans model\n",
        "with open(\"kmeans_model_final28.pkl\", \"wb\") as file:\n",
        "    pickle.dump(kmeans, file)\n",
        "print(\"KMeans model saved to 'kmeans_model.pkl'.\")\n",
        "\n",
        "# Map rules to CVEs and compute evaluation metrics\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "output_data = {\n",
        "    'combined_text': [],\n",
        "    'title': [],\n",
        "    'desc': [],\n",
        "    'cve_name': [],\n",
        "    'cve_text': [],\n",
        "    'assigned_cluster': [],\n",
        "    'similarity_score': [],\n",
        "    'correctly_mapped_cluster': []\n",
        "}\n",
        "\n",
        "correct_matches = 0\n",
        "for i, combined_embedding in enumerate(combined_embeddings):\n",
        "\n",
        "    # Predict the cluster\n",
        "    combined_cluster = kmeans.predict(combined_embedding.reshape(1, -1))[0]\n",
        "\n",
        "    # Compute similarities\n",
        "    similarities = cosine_similarity(combined_embedding.reshape(1, -1), cve_embeddings).flatten()\n",
        "    best_match_idx = np.argmax(similarities)\n",
        "    best_similarity = similarities[best_match_idx]\n",
        "\n",
        "    # Get the best matching CVE and its cluster\n",
        "    true_cve_cluster = cve_clusters[best_match_idx]\n",
        "    cve_name = cve_df.iloc[best_match_idx]['Name'] if 'Name' in cve_df.columns else 'N/A'\n",
        "    cve_text = cve_df.iloc[best_match_idx]['Processed_Text']\n",
        "\n",
        "\n",
        "    # Check if the cluster mapping is correct\n",
        "    optimal_threshold = 0.8\n",
        "    is_correct = int(best_similarity >= optimal_threshold and combined_cluster == true_cve_cluster)\n",
        "    correct_matches += is_correct\n",
        "    predicted_labels.append(combined_cluster)\n",
        "    true_labels.append(true_cve_cluster)\n",
        "\n",
        "    # Save details for analysis\n",
        "    output_data['combined_text'].append(combined_texts[i])\n",
        "    output_data['title'].append(combined_df.iloc[i]['title'])\n",
        "    output_data['desc'].append(combined_df.iloc[i]['desc'])\n",
        "    output_data['cve_name'].append(cve_name)\n",
        "    output_data['cve_text'].append(cve_text)\n",
        "    output_data['assigned_cluster'].append(combined_cluster)\n",
        "    output_data['similarity_score'].append(best_similarity)\n",
        "    output_data['correctly_mapped_cluster'].append(is_correct)\n",
        "\n",
        "\n",
        "\n",
        "# Save results to CSV\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_file = 'cnn_based_cluster_results_final28.csv'\n",
        "output_df.to_csv(output_file, index=False)\n",
        "print(f\"Results saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "0lDDRHkLUn7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn test rule to cve with cluster"
      ],
      "metadata": {
        "id": "RPsPHZxZcRnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "\n",
        "testing_data_path = 'combined_dataset_25testing.csv'\n",
        "cve_data_path = 'Processed_CVE_withSpace.csv'\n",
        "cnn_model_path = 'cnn_model_with_clustering_final128.keras'\n",
        "kmeans_model_path = 'kmeans_model_final128.pkl'\n",
        "output_file_path = 'testing_mapping_results_with_clusters_try2.csv'\n",
        "\n",
        "# Load datasets\n",
        "testing_df = pd.read_csv(testing_data_path)\n",
        "cve_df = pd.read_csv(cve_data_path)\n",
        "\n",
        "# Load models\n",
        "cnn_model = load_model(cnn_model_path)\n",
        "with open(kmeans_model_path, 'rb') as file:\n",
        "    kmeans_model = pickle.load(file)\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "# Preprocess text data\n",
        "testing_df['Best Matched Keywords'] = testing_df['Best Matched Keywords'].fillna(\"\").astype(str)\n",
        "testing_df['Processed Title + Description'] = testing_df['Processed Title + Description'].fillna(\"\").astype(str)\n",
        "testing_df['actionChannelTitle'] = testing_df['actionChannelTitle'].fillna(\"\").astype(str)\n",
        "testing_df['triggerChannelTitle'] = testing_df['triggerChannelTitle'].fillna(\"\").astype(str)\n",
        "cve_df['Processed_Text'] = cve_df['Processed_Text'].fillna(\"\").astype(str)\n",
        "\n",
        "testing_texts = (\n",
        "    testing_df['Best Matched Keywords'] + \" \" +\n",
        "    testing_df['Processed Title + Description'] + \" \" +\n",
        "    testing_df['actionChannelTitle'] + \" \" +\n",
        "    testing_df['triggerChannelTitle']\n",
        ").tolist()\n",
        "\n",
        "cve_texts = cve_df['Processed_Text'].tolist()\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(testing_texts + cve_texts)\n",
        "\n",
        "# Embedding extractor\n",
        "embedding_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-3].output)\n",
        "\n",
        "# Compute embeddings\n",
        "def compute_cnn_embeddings(texts, tokenizer, model, max_length=100):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    embeddings = model.predict(padded_sequences)\n",
        "    return embeddings\n",
        "\n",
        "print(\"Computing embeddings...\")\n",
        "testing_embeddings = compute_cnn_embeddings(testing_texts, tokenizer, embedding_extractor, max_len)\n",
        "cve_embeddings = compute_cnn_embeddings(cve_texts, tokenizer, embedding_extractor, max_len)\n",
        "print(\"Embeddings computed successfully.\")\n",
        "\n",
        "# Mapping\n",
        "output_data = {\n",
        "    'testing_text': [],\n",
        "    'title': [],\n",
        "    'desc': [],\n",
        "    'cve_name': [],\n",
        "    'cve_text': [],\n",
        "    'assigned_cluster': [],\n",
        "    'similarity_score': [],\n",
        "    'correctly_mapped_cluster': []\n",
        "}\n",
        "correct_matches = 0\n",
        "optimal_threshold = 0.8\n",
        "\n",
        "# Collect true and predicted clusters\n",
        "predicted_clusters = []\n",
        "true_clusters = []\n",
        "\n",
        "for i, test_embedding in enumerate(testing_embeddings):\n",
        "    predicted_cluster = kmeans_model.predict(test_embedding.reshape(1, -1))[0]\n",
        "    similarities = cosine_similarity(test_embedding.reshape(1, -1), cve_embeddings).flatten()\n",
        "    best_match_idx = np.argmax(similarities)\n",
        "    best_similarity = similarities[best_match_idx]\n",
        "    cve_name = cve_df.iloc[best_match_idx]['Name']\n",
        "    cve_text = cve_df.iloc[best_match_idx]['Processed_Text']\n",
        "    true_cve_cluster = kmeans_model.predict(cve_embeddings[best_match_idx].reshape(1, -1))[0]\n",
        "    is_correct = int(best_similarity >= optimal_threshold and predicted_cluster == true_cve_cluster)\n",
        "    correct_matches += is_correct\n",
        "\n",
        "    predicted_clusters.append(predicted_cluster)\n",
        "    true_clusters.append(true_cve_cluster)\n",
        "\n",
        "    output_data['testing_text'].append(testing_texts[i])\n",
        "    output_data['title'].append(testing_df.iloc[i]['title'])\n",
        "    output_data['desc'].append(testing_df.iloc[i]['desc'])\n",
        "    output_data['cve_name'].append(cve_name)\n",
        "    output_data['cve_text'].append(cve_text)\n",
        "    output_data['assigned_cluster'].append(predicted_cluster)\n",
        "    output_data['similarity_score'].append(best_similarity)\n",
        "    output_data['correctly_mapped_cluster'].append(is_correct)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_clusters, predicted_clusters)\n",
        "precision = precision_score(true_clusters, predicted_clusters, average='weighted')\n",
        "recall = recall_score(true_clusters, predicted_clusters, average='weighted')\n",
        "f1 = f1_score(true_clusters, predicted_clusters, average='weighted')\n",
        "\n",
        "print(f\"Cluster-Based Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Save results\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to '{output_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "fcU8-ZY1cVKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn test rule to cve with cluster and expert"
      ],
      "metadata": {
        "id": "R3hGnW3UceVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "\n",
        "testing_data_path = 'combined_dataset_25testing.csv'\n",
        "cve_data_path = 'Processed_CVE_withSpace.csv'\n",
        "cnn_model_path = 'cnn_model_with_clustering_final128.keras'\n",
        "kmeans_model_path = 'kmeans_model_final128.pkl'\n",
        "output_file_path = 'cnn_testing_mapping_results_with_clusters_expert.csv'\n",
        "\n",
        "# Load expert data\n",
        "expert_data = pd.read_csv('Complete_Rule_to_CVE_Mapping_with_Full_Details.csv')\n",
        "expert_labels = expert_data['Logical Match'].astype(int).values\n",
        "\n",
        "# Load datasets\n",
        "testing_df = pd.read_csv(testing_data_path)\n",
        "cve_df = pd.read_csv(cve_data_path)\n",
        "\n",
        "# Load models\n",
        "cnn_model = load_model(cnn_model_path)\n",
        "with open(kmeans_model_path, 'rb') as file:\n",
        "    kmeans_model = pickle.load(file)\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "# Preprocess text data\n",
        "testing_df['Best Matched Keywords'] = testing_df['Best Matched Keywords'].fillna(\"\").astype(str)\n",
        "testing_df['Processed Title + Description'] = testing_df['Processed Title + Description'].fillna(\"\").astype(str)\n",
        "testing_df['actionChannelTitle'] = testing_df['actionChannelTitle'].fillna(\"\").astype(str)\n",
        "testing_df['triggerChannelTitle'] = testing_df['triggerChannelTitle'].fillna(\"\").astype(str)\n",
        "cve_df['Processed_Text'] = cve_df['Processed_Text'].fillna(\"\").astype(str)\n",
        "\n",
        "testing_texts = (\n",
        "    testing_df['Best Matched Keywords'] + \" \" +\n",
        "    testing_df['Processed Title + Description'] + \" \" +\n",
        "    testing_df['actionChannelTitle'] + \" \" +\n",
        "    testing_df['triggerChannelTitle']\n",
        ").tolist()\n",
        "\n",
        "cve_texts = cve_df['Processed_Text'].tolist()\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(testing_texts + cve_texts)\n",
        "\n",
        "# Embedding extractor\n",
        "embedding_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-3].output)\n",
        "\n",
        "# Compute embeddings\n",
        "def compute_cnn_embeddings(texts, tokenizer, model, max_length=100):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    embeddings = model.predict(padded_sequences)\n",
        "    return embeddings\n",
        "\n",
        "print(\"Computing embeddings...\")\n",
        "testing_embeddings = compute_cnn_embeddings(testing_texts, tokenizer, embedding_extractor, max_len)\n",
        "cve_embeddings = compute_cnn_embeddings(cve_texts, tokenizer, embedding_extractor, max_len)\n",
        "print(\"Embeddings computed successfully.\")\n",
        "\n",
        "# Mapping\n",
        "output_data = {\n",
        "    'testing_text': [],\n",
        "    'title': [],\n",
        "    'desc': [],\n",
        "    'cve_name': [],\n",
        "    'cve_text': [],\n",
        "    'assigned_cluster': [],\n",
        "    'similarity_score': [],\n",
        "    'correctly_mapped_cluster': []\n",
        "}\n",
        "correct_matches = 0\n",
        "optimal_threshold = 0.9\n",
        "\n",
        "# Collect true and predicted clusters\n",
        "predicted_clusters = []\n",
        "true_clusters = []\n",
        "correctly_mapped_clusters = []\n",
        "\n",
        "for i, test_embedding in enumerate(testing_embeddings):\n",
        "    predicted_cluster = kmeans_model.predict(test_embedding.reshape(1, -1))[0]\n",
        "    similarities = cosine_similarity(test_embedding.reshape(1, -1), cve_embeddings).flatten()\n",
        "    best_match_idx = np.argmax(similarities)\n",
        "    best_similarity = similarities[best_match_idx]\n",
        "    cve_name = cve_df.iloc[best_match_idx]['Name']\n",
        "    cve_text = cve_df.iloc[best_match_idx]['Processed_Text']\n",
        "    true_cve_cluster = kmeans_model.predict(cve_embeddings[best_match_idx].reshape(1, -1))[0]\n",
        "\n",
        "    # Check if the mapping is correct\n",
        "    is_correct = int(best_similarity >= optimal_threshold and predicted_cluster == true_cve_cluster)\n",
        "    correctly_mapped_clusters.append(is_correct)\n",
        "\n",
        "    correct_matches += is_correct\n",
        "    predicted_clusters.append(predicted_cluster)\n",
        "    true_clusters.append(true_cve_cluster)\n",
        "\n",
        "    output_data['testing_text'].append(testing_texts[i])\n",
        "    output_data['title'].append(testing_df.iloc[i]['title'])\n",
        "    output_data['desc'].append(testing_df.iloc[i]['desc'])\n",
        "    output_data['cve_name'].append(cve_name)\n",
        "    output_data['cve_text'].append(cve_text)\n",
        "    output_data['assigned_cluster'].append(predicted_cluster)\n",
        "    output_data['similarity_score'].append(best_similarity)\n",
        "    output_data['correctly_mapped_cluster'].append(is_correct)\n",
        "\n",
        "# Save results\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv(output_file_path, index=False)\n",
        "print(f\"Results saved to '{output_file_path}'.\")\n",
        "\n",
        "\n",
        "print(\"Comparison with Expert Labels:\")\n",
        "print(classification_report(expert_labels, correctly_mapped_clusters))\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(expert_labels, correctly_mapped_clusters)\n",
        "precision = precision_score(expert_labels, correctly_mapped_clusters, average='weighted')\n",
        "recall = recall_score(expert_labels, correctly_mapped_clusters, average='weighted')\n",
        "f1 = f1_score(expert_labels, correctly_mapped_clusters, average='weighted')\n",
        "\n",
        "print(f\"Cluster-Based Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5g47x4Lwcjv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn train cve to mitre wihtout cluster"
      ],
      "metadata": {
        "id": "7nWwIKiLcybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv('training_mapping_results_cnn_model_0.2w.csv')\n",
        "mitre_data = pd.read_csv('Processed_mitre_recommidtaion.csv')\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "embedding_dim = 300\n",
        "similarity_threshold = 0.6\n",
        "keyword_weight = 0.3\n",
        "num_techniques = mitre_data['Technique'].nunique()\n",
        "\n",
        "# Combine relevant columns into text for tokenization\n",
        "train_text =(train_data['CVE Processed Text'].fillna('')+\" \"+\n",
        "             train_data['triggerChannelTitle'].fillna('') + \" \" +\n",
        "             train_data['triggerTitle'].fillna('') + \" \" +\n",
        "             train_data['actionTitle'].fillna('') + \" \" +\n",
        "             train_data['title'].fillna('') + \" \" +\n",
        "             train_data['desc'].fillna('') + \" \" +\n",
        "             train_data['Generated Topic Name'].fillna('') + \" \" +\n",
        "             train_data['Best Matched Keywords'].fillna('')\n",
        ")\n",
        "\n",
        "\n",
        "mitre_text = (mitre_data['Processed_Technique'].fillna('')+\" \"+\n",
        "             mitre_data['Tactic'].fillna('')+\" \"+\n",
        "             mitre_data['mitigation'].fillna(''))\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_text.values)\n",
        "tokenizer.fit_on_texts(mitre_text.values)\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text.values)\n",
        "mitre_sequences = tokenizer.texts_to_sequences(mitre_text.values)\n",
        "\n",
        "# Pad sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "mitre_padded = pad_sequences(mitre_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Load Word2Vec embeddings\n",
        "word2vec_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Create embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        if word in word2vec:\n",
        "            embedding_matrix[i] = word2vec[word]\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# Define the model architecture\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len,\n",
        "                            weights=[embedding_matrix], trainable=True)(input_layer)\n",
        "conv_layer = Conv1D(100, 4, activation='relu')(embedding_layer)\n",
        "batch_norm1 = BatchNormalization()(conv_layer)\n",
        "global_pool = GlobalMaxPooling1D()(batch_norm1)\n",
        "dropout_layer = Dropout(0.5)(global_pool)\n",
        "output_layer = Dense(1 , activation='sigmoid')(dropout_layer)\n",
        "\n",
        "final_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "embedding_extractor = Model(inputs=final_model.input, outputs=global_pool)\n",
        "\n",
        "# Generate embeddings for both train and CVE datasets\n",
        "train_embeddings = embedding_extractor.predict(train_padded)\n",
        "mitre_embeddings = embedding_extractor.predict(mitre_padded)\n",
        "\n",
        "\n",
        "# Compute keyword similarity\n",
        "def keyword_similarity(row_keywords, mitre_keywords):\n",
        "    train_keywords_set = set(row_keywords.lower().split())\n",
        "    mitre_keywords_set = set(mitre_keywords.lower().split())\n",
        "    intersection = train_keywords_set.intersection(mitre_keywords_set)\n",
        "    union = train_keywords_set.union(mitre_keywords_set)\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "# Calculate keyword similarities\n",
        "def compute_keyword_similarities(train_data, mitre_data):\n",
        "    keyword_similarities = np.zeros((len(train_data), len(mitre_data)))\n",
        "    for i, train_keywords in enumerate(train_data['CVE Processed Text']):\n",
        "        for j, mitre_keywords in enumerate(mitre_data['Processed_Technique']):\n",
        "            keyword_similarities[i, j] = keyword_similarity(train_keywords, mitre_keywords)\n",
        "    return keyword_similarities\n",
        "\n",
        "keyword_similarities = compute_keyword_similarities(train_data, mitre_data)\n",
        "\n",
        "# Combine embedding and keyword similarity\n",
        "combined_similarity = (1 - keyword_weight) * cosine_similarity(train_embeddings, mitre_embeddings) + keyword_weight * keyword_similarities\n",
        "best_matches = combined_similarity.argmax(axis=1)\n",
        "best_scores = combined_similarity.max(axis=1)\n",
        "\n",
        "# Generate train labels\n",
        "train_labels = (best_scores >= similarity_threshold).astype(int)\n",
        "\n",
        "# Train the final model\n",
        "final_model.fit(train_padded, train_labels, epochs=5, batch_size=32)\n",
        "\n",
        "# Save the final trained model\n",
        "final_model.save(\"final_cnn_model_for_mitre_mapping_with.keras\")\n",
        "print(\"Final model saved as 'final_cnn_model_for_mitre_mapping.keras'.\")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_matrix = cosine_similarity(train_embeddings, mitre_embeddings)\n",
        "\n",
        "# Find the top three matching techniques for each CVE\n",
        "top_three_matches = []\n",
        "for i in range(len(train_data)):\n",
        "\n",
        "    top_indices = np.argsort(similarity_matrix[i])[-3:][::-1]\n",
        "    top_scores = similarity_matrix[i, top_indices]\n",
        "\n",
        "    # Check if the top scores meet the similarity threshold\n",
        "    top_techniques = []\n",
        "    top_tactics = []\n",
        "    top_mitigations = []\n",
        "    for j in range(3):\n",
        "        if top_scores[j] >= similarity_threshold:\n",
        "            top_techniques.append(mitre_data['Technique'].iloc[top_indices[j]])\n",
        "            top_tactics.append(mitre_data['Tactic'].iloc[top_indices[j]])\n",
        "            top_mitigations.append(mitre_data['mitigation'].iloc[top_indices[j]])\n",
        "        else:\n",
        "            top_techniques.append(None)\n",
        "            top_tactics.append(None)\n",
        "            top_mitigations.append(None)\n",
        "\n",
        "    top_three_matches.append({\n",
        "        \"CVE Name\": train_data['CVE Name'].iloc[i],\n",
        "        \"CVE text\": train_data['CVE Processed Text'].iloc[i],\n",
        "        \"Top 1 Technique\": top_techniques[0],\n",
        "        \"Top 1 Tactic\": top_tactics[0],\n",
        "        \"Top 1 Mitigation\": top_mitigations[0],\n",
        "        \"Top 1 Similarity Score\": top_scores[0],\n",
        "        \"Top 2 Technique\": top_techniques[1],\n",
        "        \"Top 2 Tactic\": top_tactics[1],\n",
        "        \"Top 2 Mitigation\": top_mitigations[1],\n",
        "        \"Top 2 Similarity Score\": top_scores[1],\n",
        "        \"Top 3 Technique\": top_techniques[2],\n",
        "        \"Top 3 Tactic\": top_tactics[2],\n",
        "        \"Top 3 Mitigation\": top_mitigations[2],\n",
        "        \"Top 3 Similarity Score\": top_scores[2]\n",
        "    })\n",
        "\n",
        "# Convert the best matches to a DataFrame\n",
        "top_three_matches_df = pd.DataFrame(top_three_matches)\n",
        "\n",
        "\n",
        "print(top_three_matches_df.head())\n",
        "\n",
        "\n",
        "top_three_matches_df.to_csv(\"training_mapping_results_mitre_text.csv\", index=False)\n",
        "\n",
        "print(\"Final mapping results with top three matched techniques, tactics, mitigations, and similarity scores saved to 'training_mapping_results_top_three_matches.csv'.\")\n"
      ],
      "metadata": {
        "id": "nboFrLdAc2mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn test cve to mitre without cluster"
      ],
      "metadata": {
        "id": "1LJEIgbedGX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('final_cnn_model_for_mitre_mapping_with.keras')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load the test dataset and expert labels\n",
        "test_data = pd.read_csv('test_mapping_results_cnn_model_expert_without_cluster.csv')\n",
        "mitre_data = pd.read_csv('Processed_mitre_recommidtaion.csv')\n",
        "expert_data = pd.read_csv('CVE_Dataset_with_Binary_Logical_Match_Column.csv')\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "similarity_threshold = 0.9\n",
        "\n",
        "# Combine relevant columns into text for tokenization\n",
        "test_text = (test_data['CVE Processed Text'].fillna('') + \" \" +\n",
        "             test_data['triggerChannelTitle'].fillna('') + \" \" +\n",
        "             test_data['triggerTitle'].fillna('') + \" \" +\n",
        "             test_data['actionTitle'].fillna('') + \" \" +\n",
        "             test_data['title'].fillna('') + \" \" +\n",
        "             test_data['desc'].fillna('') + \" \" +\n",
        "             test_data['Generated Topic Name'].fillna('') + \" \" +\n",
        "             test_data['Best Matched Keywords'].fillna('')\n",
        ")\n",
        "\n",
        "mitre_text = (mitre_data['Processed_Technique'].fillna('') + \" \" +\n",
        "              mitre_data['Tactic'].fillna('') + \" \" +\n",
        "              mitre_data['mitigation'].fillna(''))\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(test_text.values)\n",
        "tokenizer.fit_on_texts(mitre_text.values)\n",
        "\n",
        "# Convert text to sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text.values)\n",
        "mitre_sequences = tokenizer.texts_to_sequences(mitre_text.values)\n",
        "\n",
        "# Pad sequences\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "mitre_padded = pad_sequences(mitre_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Generate embeddings for the test dataset\n",
        "embedding_extractor = Model(inputs=model.input, outputs=model.layers[-3].output)\n",
        "test_embeddings = embedding_extractor.predict(test_padded)\n",
        "mitre_embeddings = embedding_extractor.predict(mitre_padded)\n",
        "\n",
        "# Calculate cosine similarity between test and MITRE techniques\n",
        "similarity_matrix = cosine_similarity(test_embeddings, mitre_embeddings)\n",
        "\n",
        "# Find the top three matching techniques for each CVE in the test dataset\n",
        "top_three_matches = []\n",
        "for i in range(len(test_data)):\n",
        "\n",
        "    top_indices = np.argsort(similarity_matrix[i])[-3:][::-1]\n",
        "    top_scores = similarity_matrix[i, top_indices]\n",
        "\n",
        "    # Check if the top scores meet the similarity threshold\n",
        "    top_techniques = []\n",
        "    top_tactics = []\n",
        "    top_mitigations = []\n",
        "    for j in range(3):\n",
        "        if top_scores[j] >= similarity_threshold:\n",
        "            top_techniques.append(mitre_data['Technique'].iloc[top_indices[j]])\n",
        "            top_tactics.append(mitre_data['Tactic'].iloc[top_indices[j]])\n",
        "            top_mitigations.append(mitre_data['mitigation'].iloc[top_indices[j]])\n",
        "        else:\n",
        "            top_techniques.append(None)\n",
        "            top_tactics.append(None)\n",
        "            top_mitigations.append(None)\n",
        "\n",
        "    top_three_matches.append({\n",
        "        \"CVE Name\": test_data['CVE Name'].iloc[i],\n",
        "        \"CVE Processed Text\": test_data['CVE Processed Text'].iloc[i],\n",
        "        \"Top 1 Technique\": top_techniques[0],\n",
        "        \"Top 1 Tactic\": top_tactics[0],\n",
        "        \"Top 1 Mitigation\": top_mitigations[0],\n",
        "        \"Top 1 Similarity Score\": top_scores[0],\n",
        "        \"Top 2 Technique\": top_techniques[1],\n",
        "        \"Top 2 Tactic\": top_tactics[1],\n",
        "        \"Top 2 Mitigation\": top_mitigations[1],\n",
        "        \"Top 2 Similarity Score\": top_scores[1],\n",
        "        \"Top 3 Technique\": top_techniques[2],\n",
        "        \"Top 3 Tactic\": top_tactics[2],\n",
        "        \"Top 3 Mitigation\": top_mitigations[2],\n",
        "        \"Top 3 Similarity Score\": top_scores[2]\n",
        "    })\n",
        "\n",
        "# Convert the best matches to a DataFrame\n",
        "top_three_matches_df = pd.DataFrame(top_three_matches)\n",
        "\n",
        "\n",
        "top_three_matches_df['Label'] = top_three_matches_df['Top 1 Similarity Score'].apply(lambda x: 1 if x >= similarity_threshold else 0)\n",
        "\n",
        "# Save to CSV file\n",
        "top_three_matches_df.to_csv(\"cnn_mapping_results_with_top3_techniques_test_final1.csv\", index=False)\n",
        "\n",
        "print(\"Final mapping results with top three matched techniques, tactics, mitigations, similarity scores, and assigned labels saved to 'final_mapping_results_with_top3_techniques_and_labels.csv'.\")\n",
        "\n",
        "\n",
        "true_labels = expert_data['Logical Match']\n",
        "predicted_labels = top_three_matches_df['Label']\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "68LGNKt4dJ4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn train cve to miter with cluster"
      ],
      "metadata": {
        "id": "-2GESpAGdZ-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import joblib\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv('cnn_based_cluster_results_final128.csv')\n",
        "mitre_data = pd.read_csv('Processed_mitre_recommidtaion.csv')\n",
        "\n",
        "# Parameters\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "embedding_dim = 300\n",
        "similarity_threshold = 0.7\n",
        "num_techniques = mitre_data['Technique'].nunique()\n",
        "num_clusters = 23\n",
        "\n",
        "# Combine relevant columns into text for tokenization\n",
        "train_text = train_data['cve_text'].fillna('')\n",
        "mitre_text = mitre_data['Processed_Technique'].fillna('')\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_text.values)\n",
        "tokenizer.fit_on_texts(mitre_text.values)\n",
        "\n",
        "# Convert text to sequences for CNN input\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text.values)\n",
        "mitre_sequences = tokenizer.texts_to_sequences(mitre_text.values)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "mitre_padded = pad_sequences(mitre_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Load Word2Vec embeddings\n",
        "word2vec_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Create embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        if word in word2vec:\n",
        "            embedding_matrix[i] = word2vec[word]\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len,\n",
        "                            weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "conv_layer = Conv1D(100, 4, activation='relu')(embedding_layer)\n",
        "batch_norm1 = BatchNormalization()(conv_layer)\n",
        "global_pool = GlobalMaxPooling1D()(batch_norm1)\n",
        "dropout_layer = Dropout(0.5)(global_pool)\n",
        "output_layer = Dense(num_techniques, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "final_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the final model\n",
        "train_labels = np.zeros((len(train_data), num_techniques))\n",
        "final_model.fit(train_padded, train_labels, epochs=5, batch_size=32)\n",
        "\n",
        "# Save the final trained model\n",
        "final_model.save(\"final_cnn_model_for_mitre_mapping_23cluster7.keras\")\n",
        "print(\"Final model saved as 'final_cnn_model_for_mitre_mapping.keras'.\")\n",
        "\n",
        "# Define the embedding extractor model\n",
        "embedding_extractor = Model(inputs=final_model.input, outputs=global_pool)\n",
        "\n",
        "# Generate embeddings for the train dataset\n",
        "train_embeddings = embedding_extractor.predict(train_padded)\n",
        "\n",
        "# Apply K-Means clustering on the train embeddings\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "train_data['cluster'] = kmeans.fit_predict(train_embeddings)\n",
        "\n",
        "# Save the KMeans clustering model\n",
        "joblib.dump(kmeans, 'kmeans_model_for_cve2mitre_23clustering7.pkl')\n",
        "print(\"KMeans clustering model saved as 'kmeans_model_for_cve_clustering.pkl'.\")\n",
        "\n",
        "# Calculate cosine similarity between CVE and MITRE techniques\n",
        "mitre_embeddings = embedding_extractor.predict(mitre_padded)\n",
        "similarity_matrix = cosine_similarity(train_embeddings, mitre_embeddings)\n",
        "\n",
        "# Find the top three matching techniques for each CVE\n",
        "top_three_matches = []\n",
        "correctly_mapped_cluster = 0\n",
        "for i in range(len(train_data)):\n",
        "    # Get the indices of the top three matching techniques based on similarity score\n",
        "    top_indices = np.argsort(similarity_matrix[i])[-3:][::-1]\n",
        "    top_scores = similarity_matrix[i, top_indices]\n",
        "\n",
        "    # Store the techniques, tactics, and mitigations\n",
        "    top_techniques = []\n",
        "    top_tactics = []\n",
        "    top_mitigations = []\n",
        "    for j in range(3):\n",
        "        top_techniques.append(mitre_data['Technique'].iloc[top_indices[j]])\n",
        "        top_tactics.append(mitre_data['Tactic'].iloc[top_indices[j]])\n",
        "        top_mitigations.append(mitre_data['mitigation'].iloc[top_indices[j]])\n",
        "\n",
        "    # Check if the predicted cluster matches the true MITRE cluster\n",
        "    predicted_mitre_cluster = train_data['cluster'].iloc[i]\n",
        "    if i < len(mitre_embeddings):\n",
        "        true_mitre_cluster = kmeans.predict(mitre_embeddings[i].reshape(1, -1))[0]\n",
        "        is_correct_mitre_cluster = 1 if top_scores[j] >= similarity_threshold and predicted_mitre_cluster == true_mitre_cluster else 0\n",
        "    else:\n",
        "        is_correct_mitre_cluster = 0\n",
        "\n",
        "    correctly_mapped_cluster += is_correct_mitre_cluster\n",
        "\n",
        "    top_three_matches.append({\n",
        "        \"CVE Name\": train_data['cve_name'].iloc[i],\n",
        "        \"CVE processed text\": train_data['cve_text'].iloc[i],\n",
        "        \"Cluster\": train_data['cluster'].iloc[i],\n",
        "        \"Top 1 Technique\": top_techniques[0],\n",
        "        \"Top 1 Tactic\": top_tactics[0],\n",
        "        \"Top 1 Mitigation\": top_mitigations[0],\n",
        "        \"Top 1 Similarity Score\": top_scores[0],\n",
        "        \"Top 2 Technique\": top_techniques[1],\n",
        "        \"Top 2 Tactic\": top_tactics[1],\n",
        "        \"Top 2 Mitigation\": top_mitigations[1],\n",
        "        \"Top 2 Similarity Score\": top_scores[1],\n",
        "        \"Top 3 Technique\": top_techniques[2],\n",
        "        \"Top 3 Tactic\": top_tactics[2],\n",
        "        \"Top 3 Mitigation\": top_mitigations[2],\n",
        "        \"Top 3 Similarity Score\": top_scores[2],\n",
        "        \"Correctly Mapped Cluster\": is_correct_mitre_cluster\n",
        "    })\n",
        "\n",
        "# Convert the best matches to a DataFrame\n",
        "top_three_matches_df = pd.DataFrame(top_three_matches)\n",
        "\n",
        "\n",
        "print(top_three_matches_df.head())\n",
        "\n",
        "# Save to a new CSV file with the top three match details and cluster information\n",
        "top_three_matches_df.to_csv(\"training_mapping_results_mitre_top_three_matches_with_23clusters7.csv\", index=False)\n",
        "\n",
        "print(\"Final mapping results with top three matched techniques, tactics, mitigations, similarity scores, and cluster information saved.\")\n",
        "\n",
        "# Calculate the metrics based on the correctly mapped cluster\n",
        "correctly_mapped_percentage = (correctly_mapped_cluster / len(train_data)) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Correctly Mapped Cluster Percentage: {correctly_mapped_percentage:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mEqqqFS4ddhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn test cve to mitre with cluster"
      ],
      "metadata": {
        "id": "TzWICrfzdp0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = pd.read_csv('testing_mapping_results_with_clusters_try2.csv')\n",
        "\n",
        "# Load expert data\n",
        "expert_data = pd.read_csv('CVE_Dataset_with_Binary_Logical_Match_Column.csv')\n",
        "expert_data['Logical_Match'] = expert_data['Logical Match'].astype(int)\n",
        "\n",
        "test_data['cve_text'] = test_data['cve_text'].fillna('')\n",
        "\n",
        "# Load the trained CNN model\n",
        "cnn_model = load_model('final_cnn_model_for_mitre_mapping_23cluster7.keras')\n",
        "print(f\"CNN model loaded\")\n",
        "\n",
        "# Load the saved KMeans model (the one you trained on the MITRE data)\n",
        "kmeans = joblib.load('kmeans_model_for_cve2mitre_23clustering7.pkl')\n",
        "print(f\"KMeans model loaded\")\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "mitre_data = pd.read_csv('Processed_mitre_recommidtaion.csv')\n",
        "mitre_text = mitre_data['Processed_Technique'].fillna('')\n",
        "tokenizer.fit_on_texts(mitre_text)\n",
        "\n",
        "# Define parameters\n",
        "max_len = 100\n",
        "embedding_dim = 300\n",
        "\n",
        "# Convert the test text to sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['cve_text'].values)\n",
        "\n",
        "# Check if test_sequences is empty or None\n",
        "if not test_sequences or any(len(seq) == 0 for seq in test_sequences):\n",
        "    raise ValueError(\"Test data tokenization resulted in empty sequences.\")\n",
        "\n",
        "# Pad the test sequences to ensure uniform input length\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Generate embeddings for the test dataset using the trained CNN model\n",
        "embedding_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
        "test_embeddings = embedding_extractor.predict(test_padded)\n",
        "\n",
        "# Scale the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_embeddings = scaler.fit_transform(test_embeddings)\n",
        "\n",
        "# Use the saved KMeans model\n",
        "test_clusters = kmeans.predict(scaled_embeddings)\n",
        "\n",
        "# Load the MITRE data to get the techniques for mapping\n",
        "mitre_text = mitre_data['Processed_Technique'].fillna('')\n",
        "mitre_sequences = tokenizer.texts_to_sequences(mitre_text.values)\n",
        "mitre_padded = pad_sequences(mitre_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Calculate the cosine similarity between the test embeddings and the MITRE embeddings\n",
        "mitre_embeddings = embedding_extractor.predict(mitre_padded)\n",
        "similarity_matrix = cosine_similarity(test_embeddings, mitre_embeddings)\n",
        "\n",
        "# Set threshold value for cosine similarity\n",
        "similarity_threshold = 0.63\n",
        "\n",
        "# Find the top three matching techniques for each CVE in the test dataset\n",
        "top_three_matches = []\n",
        "correctly_mapped_cluster = 0\n",
        "\n",
        "# Initialize lists for true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "\n",
        "    top_indices = np.argsort(similarity_matrix[i])[-3:][::-1]\n",
        "    top_scores = similarity_matrix[i, top_indices]\n",
        "\n",
        "    # Get the top 3 techniques and check if they meet the threshold\n",
        "    top_techniques = []\n",
        "    top_tactics = []\n",
        "    top_mitigations = []\n",
        "    for j in range(3):\n",
        "        top_techniques.append(mitre_data['Technique'].iloc[top_indices[j]])\n",
        "        top_tactics.append(mitre_data['Tactic'].iloc[top_indices[j]])\n",
        "        top_mitigations.append(mitre_data['mitigation'].iloc[top_indices[j]])\n",
        "\n",
        "    # Check if the predicted cluster matches the true MITRE cluster\n",
        "    predicted_mitre_cluster = test_clusters[i]\n",
        "    true_mitre_cluster = kmeans.predict(mitre_embeddings[top_indices[0]].reshape(1, -1))[0]\n",
        "    is_correct_mitre_cluster = 1 if top_scores[j] >= similarity_threshold and predicted_mitre_cluster == true_mitre_cluster else 0\n",
        "    correctly_mapped_cluster += is_correct_mitre_cluster\n",
        "\n",
        "    true_labels.append(expert_data['Logical_Match'].iloc[i])\n",
        "    predicted_labels.append(is_correct_mitre_cluster)\n",
        "\n",
        "    top_three_matches.append({\n",
        "        \"CVE Name\": test_data['cve_name'].iloc[i],\n",
        "        \"CVE processed text\": test_data['cve_text'].iloc[i],\n",
        "        \"Cluster\": test_clusters[i],\n",
        "        \"Top 1 Technique\": top_techniques[0],\n",
        "        \"Top 1 Tactic\": top_tactics[0],\n",
        "        \"Top 1 Mitigation\": top_mitigations[0],\n",
        "        \"Top 1 Similarity Score\": top_scores[0],\n",
        "        \"Top 2 Technique\": top_techniques[1],\n",
        "        \"Top 2 Tactic\": top_tactics[1],\n",
        "        \"Top 2 Mitigation\": top_mitigations[1],\n",
        "        \"Top 2 Similarity Score\": top_scores[1],\n",
        "        \"Top 3 Technique\": top_techniques[2],\n",
        "        \"Top 3 Tactic\": top_tactics[2],\n",
        "        \"Top 3 Mitigation\": top_mitigations[2],\n",
        "        \"Top 3 Similarity Score\": top_scores[2],\n",
        "        \"Correctly Mapped Cluster\": is_correct_mitre_cluster\n",
        "    })\n",
        "\n",
        "# Convert the best matches to a DataFrame\n",
        "top_three_matches_df = pd.DataFrame(top_three_matches)\n",
        "\n",
        "# Save to CSV file\n",
        "top_three_matches_df.to_csv(\"testing_mapping_cnn_results_mitre_cluster_final_one.csv\", index=False)\n",
        "print(\"Final mapping results with top three matched techniques, tactics, mitigations, similarity scores, and cluster information saved.\")\n",
        "\n",
        "# Calculate the metrics based on the correctly mapped clusters\n",
        "correctly_mapped_percentage = (correctly_mapped_cluster / len(test_data)) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Correctly Mapped Cluster Percentage: {correctly_mapped_percentage:.2f}%\")\n",
        "\n",
        "# Align with expert labels\n",
        "expert_labels = expert_data['Logical_Match']\n",
        "\n",
        "# Evaluate results\n",
        "accuracy_expert = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Cluster-Based Accuracy: {correctly_mapped_percentage:.2f}%\")\n",
        "print(f\"Accuracy Compared to Expert Labels: {accuracy_expert:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Compare with the expert dataset\n",
        "correct_matches_expert = sum(top_three_matches_df['Correctly Mapped Cluster'] == expert_data['Logical Match'])\n",
        "accuracy_expert = correct_matches_expert / len(top_three_matches_df) * 100\n",
        "\n",
        "print(f\"Accuracy Compared to Expert Labels: {accuracy_expert:.2f}%\")\n",
        "print(classification_report(expert_data['Logical Match'], top_three_matches_df['Correctly Mapped Cluster']))\n",
        "\n"
      ],
      "metadata": {
        "id": "m8zLQ_SNdsdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}